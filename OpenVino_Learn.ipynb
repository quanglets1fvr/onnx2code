{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4e43da242832441f9037124916b62a56": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_89f840c9cee646d5ab309d289c5b97a2",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "Statistics collection \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[38;2;0;104;181m300/300\u001b[0m • \u001b[38;2;0;104;181m0:00:27\u001b[0m • \u001b[38;2;0;104;181m0:00:00\u001b[0m\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Statistics collection <span style=\"color: #729c1f; text-decoration-color: #729c1f\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #800080; text-decoration-color: #800080\">100%</span> <span style=\"color: #0068b5; text-decoration-color: #0068b5\">300/300</span> • <span style=\"color: #0068b5; text-decoration-color: #0068b5\">0:00:27</span> • <span style=\"color: #0068b5; text-decoration-color: #0068b5\">0:00:00</span>\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "89f840c9cee646d5ab309d289c5b97a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a9eaa0ab60447f9a9cfbc0eed504f3c": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_a448f11b8e774826a7049c01a42da717",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "Applying Fast Bias correction \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[38;2;0;104;181m36/36\u001b[0m • \u001b[38;2;0;104;181m0:00:01\u001b[0m • \u001b[38;2;0;104;181m0:00:00\u001b[0m\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Applying Fast Bias correction <span style=\"color: #729c1f; text-decoration-color: #729c1f\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #800080; text-decoration-color: #800080\">100%</span> <span style=\"color: #0068b5; text-decoration-color: #0068b5\">36/36</span> • <span style=\"color: #0068b5; text-decoration-color: #0068b5\">0:00:01</span> • <span style=\"color: #0068b5; text-decoration-color: #0068b5\">0:00:00</span>\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "a448f11b8e774826a7049c01a42da717": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1s4A5oWfYXF4",
        "outputId": "35de033c-6ec6-4a20-d4b5-005ec149d5d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openvino\n",
            "  Downloading openvino-2024.2.0-15519-cp310-cp310-manylinux2014_x86_64.whl (38.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.7/38.7 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.16.6 in /usr/local/lib/python3.10/dist-packages (from openvino) (1.25.2)\n",
            "Collecting openvino-telemetry>=2023.2.1 (from openvino)\n",
            "  Downloading openvino_telemetry-2024.1.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from openvino) (24.1)\n",
            "Installing collected packages: openvino-telemetry, openvino\n",
            "Successfully installed openvino-2024.2.0 openvino-telemetry-2024.1.0\n"
          ]
        }
      ],
      "source": [
        "pip install -U openvino"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nncf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2c90kBEY1MA",
        "outputId": "a3adb22f-45c7-4c69-e5b6-71e18e750fef"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nncf\n",
            "  Downloading nncf-2.11.0-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jsonschema>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from nncf) (4.19.2)\n",
            "Collecting jstyleson>=0.0.2 (from nncf)\n",
            "  Downloading jstyleson-0.0.2.tar.gz (2.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: natsort>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from nncf) (8.4.0)\n",
            "Requirement already satisfied: networkx<=3.3,>=2.6 in /usr/local/lib/python3.10/dist-packages (from nncf) (3.3)\n",
            "Collecting ninja<1.12,>=1.10.0.post2 (from nncf)\n",
            "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<1.27,>=1.19.1 in /usr/local/lib/python3.10/dist-packages (from nncf) (1.25.2)\n",
            "Requirement already satisfied: openvino-telemetry>=2023.2.0 in /usr/local/lib/python3.10/dist-packages (from nncf) (2024.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from nncf) (24.1)\n",
            "Requirement already satisfied: pandas<2.3,>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from nncf) (2.0.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from nncf) (5.9.5)\n",
            "Requirement already satisfied: pydot>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nncf) (1.4.2)\n",
            "Collecting pymoo>=0.6.0.1 (from nncf)\n",
            "  Downloading pymoo-0.6.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: rich>=13.5.2 in /usr/local/lib/python3.10/dist-packages (from nncf) (13.7.1)\n",
            "Requirement already satisfied: scikit-learn>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from nncf) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from nncf) (1.11.4)\n",
            "Requirement already satisfied: tabulate>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from nncf) (0.9.0)\n",
            "Requirement already satisfied: tqdm>=4.54.1 in /usr/local/lib/python3.10/dist-packages (from nncf) (4.66.4)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.2.0->nncf) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.2.0->nncf) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.2.0->nncf) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.2.0->nncf) (0.19.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<2.3,>=1.1.5->nncf) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.3,>=1.1.5->nncf) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.3,>=1.1.5->nncf) (2024.1)\n",
            "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.10/dist-packages (from pydot>=1.4.1->nncf) (3.1.2)\n",
            "Requirement already satisfied: matplotlib>=3 in /usr/local/lib/python3.10/dist-packages (from pymoo>=0.6.0.1->nncf) (3.7.1)\n",
            "Requirement already satisfied: autograd>=1.4 in /usr/local/lib/python3.10/dist-packages (from pymoo>=0.6.0.1->nncf) (1.6.2)\n",
            "Collecting cma==3.2.2 (from pymoo>=0.6.0.1->nncf)\n",
            "  Downloading cma-3.2.2-py2.py3-none-any.whl (249 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m249.1/249.1 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting alive-progress (from pymoo>=0.6.0.1->nncf)\n",
            "  Downloading alive_progress-3.1.5-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dill (from pymoo>=0.6.0.1->nncf)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting Deprecated (from pymoo>=0.6.0.1->nncf)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13.5.2->nncf) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13.5.2->nncf) (2.16.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.0->nncf) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.0->nncf) (3.5.0)\n",
            "Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.10/dist-packages (from autograd>=1.4->pymoo>=0.6.0.1->nncf) (0.18.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=13.5.2->nncf) (0.1.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->pymoo>=0.6.0.1->nncf) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->pymoo>=0.6.0.1->nncf) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->pymoo>=0.6.0.1->nncf) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->pymoo>=0.6.0.1->nncf) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3->pymoo>=0.6.0.1->nncf) (9.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<2.3,>=1.1.5->nncf) (1.16.0)\n",
            "Collecting about-time==4.2.1 (from alive-progress->pymoo>=0.6.0.1->nncf)\n",
            "  Downloading about_time-4.2.1-py3-none-any.whl (13 kB)\n",
            "Collecting grapheme==0.6.0 (from alive-progress->pymoo>=0.6.0.1->nncf)\n",
            "  Downloading grapheme-0.6.0.tar.gz (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from Deprecated->pymoo>=0.6.0.1->nncf) (1.14.1)\n",
            "Building wheels for collected packages: jstyleson, grapheme\n",
            "  Building wheel for jstyleson (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jstyleson: filename=jstyleson-0.0.2-py3-none-any.whl size=2385 sha256=33b33eeee7360c9afaf51a8d4bce75d0edcebbe6d51e376a19f6ff01c678ee7b\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/51/c6/a1e751db88203e11c6d9ffe4683ca3d8c14b1479639bec1006\n",
            "  Building wheel for grapheme (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for grapheme: filename=grapheme-0.6.0-py3-none-any.whl size=210078 sha256=1a6d0dc82fca2b50b0c622120c4b27dfff990548f08eaa13617c408fb3c41e95\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/e1/49/37e6bde9886439057450c494a79b0bef8bbe897a54aebfc757\n",
            "Successfully built jstyleson grapheme\n",
            "Installing collected packages: ninja, jstyleson, grapheme, dill, Deprecated, cma, about-time, alive-progress, pymoo, nncf\n",
            "Successfully installed Deprecated-1.2.14 about-time-4.2.1 alive-progress-3.1.5 cma-3.2.2 dill-0.3.8 grapheme-0.6.0 jstyleson-0.0.2 ninja-1.11.1.1 nncf-2.11.0 pymoo-0.6.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openvino as ov\n",
        "import torchvision\n",
        "import torch\n",
        "import nncf\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vre7QTKtZMb5",
        "outputId": "acb9efad-a307-4262-cf43-a62e2e50eacb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:nncf:NNCF initialized successfully. Supported frameworks detected: torch, tensorflow, openvino\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fastdownload\n",
        "import sklearn"
      ],
      "metadata": {
        "id": "38t6s4gSZdLP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "from typing import List, Optional\n",
        "\n",
        "import numpy as np\n",
        "import openvino as ov\n",
        "import torch\n",
        "from fastdownload import FastDownload\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from tqdm import tqdm\n",
        "\n",
        "import nncf\n",
        "\n",
        "# ROOT = Path(__file__).parent.resolve()\n",
        "MODEL_URL = \"https://huggingface.co/alexsu52/mobilenet_v2_imagenette/resolve/main/openvino_model.tgz\"\n",
        "MODEL_PATH = \"~/.cache/nncf/models\"\n",
        "DATASET_URL = \"https://s3.amazonaws.com/fast-ai-imageclas/imagenette2-320.tgz\"\n",
        "DATASET_PATH = \"~/.cache/nncf/datasets\"\n",
        "DATASET_CLASSES = 10\n",
        "\n",
        "\n",
        "def download(url: str, path: str) -> Path:\n",
        "    downloader = FastDownload(base=path, archive=\"downloaded\", data=\"extracted\")\n",
        "    return downloader.get(url)\n",
        "\n",
        "\n",
        "def validate(model: ov.Model, val_loader: torch.utils.data.DataLoader) -> float:\n",
        "    predictions = []\n",
        "    references = []\n",
        "\n",
        "    compiled_model = ov.compile_model(model, device_name=\"CPU\")\n",
        "    output = compiled_model.outputs[0]\n",
        "\n",
        "    for images, target in tqdm(val_loader):\n",
        "        pred = compiled_model(images)[output]\n",
        "        predictions.append(np.argmax(pred, axis=1))\n",
        "        references.append(target)\n",
        "\n",
        "    predictions = np.concatenate(predictions, axis=0)\n",
        "    references = np.concatenate(references, axis=0)\n",
        "    return accuracy_score(predictions, references)\n",
        "\n",
        "\n",
        "def run_benchmark(model_path: Path, shape: Optional[List[int]] = None, verbose: bool = True) -> float:\n",
        "    command = f\"benchmark_app -m {model_path} -d CPU -api async -t 15\"\n",
        "    if shape is not None:\n",
        "        command += f' -shape [{\",\".join(str(x) for x in shape)}]'\n",
        "    cmd_output = subprocess.check_output(command, shell=True)  # nosec\n",
        "    if verbose:\n",
        "        print(*str(cmd_output).split(\"\\\\n\")[-9:-1], sep=\"\\n\")\n",
        "    match = re.search(r\"Throughput\\: (.+?) FPS\", str(cmd_output))\n",
        "    return float(match.group(1))\n",
        "\n",
        "\n",
        "def get_model_size(ir_path: Path, m_type: str = \"Mb\", verbose: bool = True) -> float:\n",
        "    xml_size = os.path.getsize(ir_path)\n",
        "    bin_size = os.path.getsize(os.path.splitext(ir_path)[0] + \".bin\")\n",
        "    for t in [\"bytes\", \"Kb\", \"Mb\"]:\n",
        "        if m_type == t:\n",
        "            break\n",
        "        xml_size /= 1024\n",
        "        bin_size /= 1024\n",
        "    model_size = xml_size + bin_size\n",
        "    if verbose:\n",
        "        print(f\"Model graph (xml):   {xml_size:.3f} Mb\")\n",
        "        print(f\"Model weights (bin): {bin_size:.3f} Mb\")\n",
        "        print(f\"Model size:          {model_size:.3f} Mb\")\n",
        "    return model_size\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "# Create an OpenVINO model and dataset\n",
        "\n",
        "dataset_path = download(DATASET_URL, DATASET_PATH)\n",
        "\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "val_dataset = datasets.ImageFolder(\n",
        "    root=dataset_path / \"val\",\n",
        "    transform=transforms.Compose(\n",
        "        [\n",
        "            transforms.Resize(256),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            normalize,\n",
        "        ]\n",
        "    ),\n",
        ")\n",
        "val_data_loader = torch.utils.data.DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "path_to_model = download(MODEL_URL, MODEL_PATH)\n",
        "ov_model = ov.Core().read_model(path_to_model / \"mobilenet_v2_fp32.xml\")\n",
        "\n",
        "###############################################################################\n",
        "# Quantize an OpenVINO model\n",
        "#\n",
        "# The transformation function transforms a data item into model input data.\n",
        "#\n",
        "# To validate the transform function use the following code:\n",
        "# >> for data_item in val_loader:\n",
        "# >>    model(transform_fn(data_item))\n",
        "\n",
        "\n",
        "def transform_fn(data_item):\n",
        "    images, _ = data_item\n",
        "    return images\n",
        "\n",
        "\n",
        "# The calibration dataset is a small, no label, representative dataset\n",
        "# (~100-500 samples) that is used to estimate the range, i.e. (min, max) of all\n",
        "# floating point activation tensors in the model, to initialize the quantization\n",
        "# parameters.\n",
        "#\n",
        "# The easiest way to define a calibration dataset is to use a training or\n",
        "# validation dataset and a transformation function to remove labels from the data\n",
        "# item and prepare model input data. The quantize method uses a small subset\n",
        "# (default: 300 samples) of the calibration dataset.\n",
        "\n",
        "calibration_dataset = nncf.Dataset(val_data_loader, transform_fn)\n",
        "ov_quantized_model = nncf.quantize(ov_model, calibration_dataset)\n",
        "\n",
        "###############################################################################\n",
        "# Benchmark performance, calculate compression rate and validate accuracy\n",
        "\n",
        "fp32_ir_path = ROOT / \"mobilenet_v2_fp32.xml\"\n",
        "ov.save_model(ov_model, fp32_ir_path, compress_to_fp16=False)\n",
        "print(f\"[1/7] Save FP32 model: {fp32_ir_path}\")\n",
        "fp32_model_size = get_model_size(fp32_ir_path, verbose=True)\n",
        "\n",
        "int8_ir_path = ROOT / \"mobilenet_v2_int8.xml\"\n",
        "ov.save_model(ov_quantized_model, int8_ir_path)\n",
        "print(f\"[2/7] Save INT8 model: {int8_ir_path}\")\n",
        "int8_model_size = get_model_size(int8_ir_path, verbose=True)\n",
        "\n",
        "print(\"[3/7] Benchmark FP32 model:\")\n",
        "fp32_fps = run_benchmark(fp32_ir_path, shape=[1, 3, 224, 224], verbose=True)\n",
        "print(\"[4/7] Benchmark INT8 model:\")\n",
        "int8_fps = run_benchmark(int8_ir_path, shape=[1, 3, 224, 224], verbose=True)\n",
        "\n",
        "print(\"[5/7] Validate OpenVINO FP32 model:\")\n",
        "fp32_top1 = validate(ov_model, val_data_loader)\n",
        "print(f\"Accuracy @ top1: {fp32_top1:.3f}\")\n",
        "\n",
        "print(\"[6/7] Validate OpenVINO INT8 model:\")\n",
        "int8_top1 = validate(ov_quantized_model, val_data_loader)\n",
        "print(f\"Accuracy @ top1: {int8_top1:.3f}\")\n",
        "\n",
        "print(\"[7/7] Report:\")\n",
        "print(f\"Accuracy drop: {fp32_top1 - int8_top1:.3f}\")\n",
        "print(f\"Model compression rate: {fp32_model_size / int8_model_size:.3f}\")\n",
        "# https://docs.openvino.ai/latest/openvino_docs_optimization_guide_dldt_optimization_guide.html\n",
        "print(f\"Performance speed up (throughput mode): {int8_fps / fp32_fps:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 674,
          "referenced_widgets": [
            "4e43da242832441f9037124916b62a56",
            "89f840c9cee646d5ab309d289c5b97a2",
            "3a9eaa0ab60447f9a9cfbc0eed504f3c",
            "a448f11b8e774826a7049c01a42da717"
          ]
        },
        "id": "sH04NvHsaOTV",
        "outputId": "661973d7-0c0e-45b7-b9b7-13f841fc5a2b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4e43da242832441f9037124916b62a56"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3a9eaa0ab60447f9a9cfbc0eed504f3c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1/7] Save FP32 model: /content/mobilenet_v2_fp32.xml\n",
            "Model graph (xml):   0.138 Mb\n",
            "Model weights (bin): 8.467 Mb\n",
            "Model size:          8.605 Mb\n",
            "[2/7] Save INT8 model: /content/mobilenet_v2_int8.xml\n",
            "Model graph (xml):   0.348 Mb\n",
            "Model weights (bin): 2.267 Mb\n",
            "Model size:          2.616 Mb\n",
            "[3/7] Benchmark FP32 model:\n",
            "[ INFO ] Count:            1214 iterations\n",
            "[ INFO ] Duration:         15034.52 ms\n",
            "[ INFO ] Latency:\n",
            "[ INFO ]    Median:        20.37 ms\n",
            "[ INFO ]    Average:       24.60 ms\n",
            "[ INFO ]    Min:           15.32 ms\n",
            "[ INFO ]    Max:           67.69 ms\n",
            "[ INFO ] Throughput:   80.75 FPS\n",
            "[4/7] Benchmark INT8 model:\n",
            "[ INFO ] Count:            1538 iterations\n",
            "[ INFO ] Duration:         15015.76 ms\n",
            "[ INFO ] Latency:\n",
            "[ INFO ]    Median:        16.31 ms\n",
            "[ INFO ]    Average:       19.36 ms\n",
            "[ INFO ]    Min:           10.57 ms\n",
            "[ INFO ]    Max:           59.69 ms\n",
            "[ INFO ] Throughput:   102.43 FPS\n",
            "[5/7] Validate OpenVINO FP32 model:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3925/3925 [01:23<00:00, 47.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy @ top1: 0.986\n",
            "[6/7] Validate OpenVINO INT8 model:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3925/3925 [01:12<00:00, 53.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy @ top1: 0.985\n",
            "[7/7] Report:\n",
            "Accuracy drop: 0.002\n",
            "Model compression rate: 3.290\n",
            "Performance speed up (throughput mode): 1.268\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "Wk30N7lCboMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if '__file__' not in globals():\n",
        "    __file__ = os.path.abspath(\"OpenVino_Learn.ipynb\")  # Replace 'script.py' with a mock script name\n",
        "ROOT = Path(__file__).parent.resolve()"
      ],
      "metadata": {
        "id": "1PJxGGvGcSVr"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ROOT / \"mobilenet_v2_fp32.xml\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7n319Oedv1Q",
        "outputId": "4d2b89f9-f596-4ac6-8d73-49d10cc88826"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Path('/content/mobilenet_v2_fp32.xml')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics==8.0.170 onnx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hv1h8WNVlPEX",
        "outputId": "8d488c95-0a7f-4142-ed94-c6ffc0c0c5b0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics==8.0.170\n",
            "  Downloading ultralytics-8.0.170-py3-none-any.whl (614 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m614.2/614.2 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnx\n",
            "  Downloading onnx-1.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.0.170) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.22.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.0.170) (1.25.2)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.0.170) (4.8.0.76)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.0.170) (9.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.0.170) (6.0.1)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.0.170) (2.31.0)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.0.170) (1.11.4)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.0.170) (2.3.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.0.170) (0.18.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.0.170) (4.66.4)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.0.170) (2.0.3)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.0.170) (0.13.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.0.170) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics==8.0.170) (9.0.0)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx) (3.20.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics==8.0.170) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics==8.0.170) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics==8.0.170) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics==8.0.170) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics==8.0.170) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics==8.0.170) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics==8.0.170) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics==8.0.170) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics==8.0.170) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics==8.0.170) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics==8.0.170) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics==8.0.170) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics==8.0.170) (2024.7.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics==8.0.170) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics==8.0.170) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics==8.0.170) (1.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics==8.0.170) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics==8.0.170) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics==8.0.170) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.8.0->ultralytics==8.0.170)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.8.0->ultralytics==8.0.170)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.8.0->ultralytics==8.0.170)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.8.0->ultralytics==8.0.170)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.8.0->ultralytics==8.0.170)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.8.0->ultralytics==8.0.170)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.8.0->ultralytics==8.0.170)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.8.0->ultralytics==8.0.170)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.8.0->ultralytics==8.0.170)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.8.0->ultralytics==8.0.170)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.8.0->ultralytics==8.0.170)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics==8.0.170) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.8.0->ultralytics==8.0.170)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.2.2->ultralytics==8.0.170) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics==8.0.170) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->ultralytics==8.0.170) (1.3.0)\n",
            "Installing collected packages: onnx, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, ultralytics\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 onnx-1.16.1 ultralytics-8.0.170\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import subprocess\n",
        "from functools import partial\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import openvino as ov\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from ultralytics.cfg import get_cfg\n",
        "from ultralytics.data.converter import coco80_to_coco91_class\n",
        "from ultralytics.data.utils import check_det_dataset\n",
        "from ultralytics.engine.validator import BaseValidator as Validator\n",
        "from ultralytics.models.yolo import YOLO\n",
        "from ultralytics.utils import DATASETS_DIR\n",
        "from ultralytics.utils import DEFAULT_CFG\n",
        "from ultralytics.utils import ops\n",
        "from ultralytics.utils.metrics import ConfusionMatrix\n",
        "\n",
        "import nncf\n",
        "\n",
        "# ROOT = Path(__file__).parent.resolve()\n",
        "\n",
        "\n",
        "def validate(\n",
        "    model: ov.Model, data_loader: torch.utils.data.DataLoader, validator: Validator, num_samples: int = None\n",
        ") -> Tuple[Dict, int, int]:\n",
        "    validator.seen = 0\n",
        "    validator.jdict = []\n",
        "    validator.stats = []\n",
        "    validator.batch_i = 1\n",
        "    validator.confusion_matrix = ConfusionMatrix(nc=validator.nc)\n",
        "    model.reshape({0: [1, 3, -1, -1]})\n",
        "    compiled_model = ov.compile_model(model, device_name=\"CPU\")\n",
        "    num_outputs = len(model.outputs)\n",
        "    for batch_i, batch in enumerate(data_loader):\n",
        "        if num_samples is not None and batch_i == num_samples:\n",
        "            break\n",
        "        batch = validator.preprocess(batch)\n",
        "        results = compiled_model(batch[\"img\"])\n",
        "        if num_outputs == 1:\n",
        "            preds = torch.from_numpy(results[compiled_model.output(0)])\n",
        "        else:\n",
        "            preds = [\n",
        "                torch.from_numpy(results[compiled_model.output(0)]),\n",
        "                torch.from_numpy(results[compiled_model.output(1)]),\n",
        "            ]\n",
        "        preds = validator.postprocess(preds)\n",
        "        validator.update_metrics(preds, batch)\n",
        "    stats = validator.get_stats()\n",
        "    return stats, validator.seen, validator.nt_per_class.sum()\n",
        "\n",
        "\n",
        "def print_statistics(stats: np.ndarray, total_images: int, total_objects: int) -> None:\n",
        "    print(\"Metrics(Box):\")\n",
        "    mp, mr, map50, mean_ap = (\n",
        "        stats[\"metrics/precision(B)\"],\n",
        "        stats[\"metrics/recall(B)\"],\n",
        "        stats[\"metrics/mAP50(B)\"],\n",
        "        stats[\"metrics/mAP50-95(B)\"],\n",
        "    )\n",
        "    s = (\"%20s\" + \"%12s\" * 6) % (\"Class\", \"Images\", \"Labels\", \"Precision\", \"Recall\", \"mAP@.5\", \"mAP@.5:.95\")\n",
        "    print(s)\n",
        "    pf = \"%20s\" + \"%12i\" * 2 + \"%12.3g\" * 4  # print format\n",
        "    print(pf % (\"all\", total_images, total_objects, mp, mr, map50, mean_ap))\n",
        "\n",
        "    # print the mask metrics for segmentation\n",
        "    if \"metrics/precision(M)\" in stats:\n",
        "        print(\"Metrics(Mask):\")\n",
        "        s_mp, s_mr, s_map50, s_mean_ap = (\n",
        "            stats[\"metrics/precision(M)\"],\n",
        "            stats[\"metrics/recall(M)\"],\n",
        "            stats[\"metrics/mAP50(M)\"],\n",
        "            stats[\"metrics/mAP50-95(M)\"],\n",
        "        )\n",
        "        # Print results\n",
        "        s = (\"%20s\" + \"%12s\" * 6) % (\"Class\", \"Images\", \"Labels\", \"Precision\", \"Recall\", \"mAP@.5\", \"mAP@.5:.95\")\n",
        "        print(s)\n",
        "        pf = \"%20s\" + \"%12i\" * 2 + \"%12.3g\" * 4  # print format\n",
        "        print(pf % (\"all\", total_images, total_objects, s_mp, s_mr, s_map50, s_mean_ap))\n",
        "\n",
        "\n",
        "def prepare_validation(model: YOLO, args: Any) -> Tuple[Validator, torch.utils.data.DataLoader]:\n",
        "    validator = model.smart_load(\"validator\")(args)\n",
        "    validator.data = check_det_dataset(args.data)\n",
        "    dataset = validator.data[\"val\"]\n",
        "    print(f\"{dataset}\")\n",
        "\n",
        "    data_loader = validator.get_dataloader(f\"{DATASETS_DIR}/coco128-seg\", 1)\n",
        "\n",
        "    validator = model.smart_load(\"validator\")(args)\n",
        "\n",
        "    validator.is_coco = True\n",
        "    validator.class_map = coco80_to_coco91_class()\n",
        "    validator.names = model.model.names\n",
        "    validator.metrics.names = validator.names\n",
        "    validator.nc = model.model.model[-1].nc\n",
        "    validator.nm = 32\n",
        "    validator.process = ops.process_mask\n",
        "    validator.plot_masks = []\n",
        "\n",
        "    return validator, data_loader\n",
        "\n",
        "\n",
        "def benchmark_performance(model_path, config) -> float:\n",
        "    command = f\"benchmark_app -m {model_path} -d CPU -api async -t 30\"\n",
        "    command += f' -shape \"[1,3,{config.imgsz},{config.imgsz}]\"'\n",
        "    cmd_output = subprocess.check_output(command, shell=True)  # nosec\n",
        "\n",
        "    match = re.search(r\"Throughput\\: (.+?) FPS\", str(cmd_output))\n",
        "    return float(match.group(1))\n",
        "\n",
        "\n",
        "def prepare_openvino_model(model: YOLO, model_name: str) -> Tuple[ov.Model, Path]:\n",
        "    ir_model_path = Path(f\"{ROOT}/{model_name}_openvino_model/{model_name}.xml\")\n",
        "    if not ir_model_path.exists():\n",
        "        onnx_model_path = Path(f\"{ROOT}/{model_name}.onnx\")\n",
        "        if not onnx_model_path.exists():\n",
        "            model.export(format=\"onnx\", dynamic=True, half=False)\n",
        "\n",
        "        ov.save_model(ov.convert_model(onnx_model_path), ir_model_path)\n",
        "    return ov.Core().read_model(ir_model_path), ir_model_path\n",
        "\n",
        "\n",
        "def quantize_ac(model: ov.Model, data_loader: torch.utils.data.DataLoader, validator_ac: Validator) -> ov.Model:\n",
        "    def transform_fn(data_item: Dict):\n",
        "        input_tensor = validator_ac.preprocess(data_item)[\"img\"].numpy()\n",
        "        return input_tensor\n",
        "\n",
        "    def validation_ac(\n",
        "        compiled_model: ov.CompiledModel,\n",
        "        validation_loader: torch.utils.data.DataLoader,\n",
        "        validator: Validator,\n",
        "        num_samples: int = None,\n",
        "    ) -> float:\n",
        "        validator.seen = 0\n",
        "        validator.jdict = []\n",
        "        validator.stats = []\n",
        "        validator.batch_i = 1\n",
        "        validator.confusion_matrix = ConfusionMatrix(nc=validator.nc)\n",
        "        num_outputs = len(compiled_model.outputs)\n",
        "\n",
        "        counter = 0\n",
        "        for batch_i, batch in enumerate(validation_loader):\n",
        "            if num_samples is not None and batch_i == num_samples:\n",
        "                break\n",
        "            batch = validator.preprocess(batch)\n",
        "            results = compiled_model(batch[\"img\"])\n",
        "            if num_outputs == 1:\n",
        "                preds = torch.from_numpy(results[compiled_model.output(0)])\n",
        "            else:\n",
        "                preds = [\n",
        "                    torch.from_numpy(results[compiled_model.output(0)]),\n",
        "                    torch.from_numpy(results[compiled_model.output(1)]),\n",
        "                ]\n",
        "            preds = validator.postprocess(preds)\n",
        "            validator.update_metrics(preds, batch)\n",
        "            counter += 1\n",
        "        stats = validator.get_stats()\n",
        "        if num_outputs == 1:\n",
        "            stats_metrics = stats[\"metrics/mAP50-95(B)\"]\n",
        "        else:\n",
        "            stats_metrics = stats[\"metrics/mAP50-95(M)\"]\n",
        "        print(f\"Validate: dataset length = {counter}, metric value = {stats_metrics:.3f}\")\n",
        "        return stats_metrics\n",
        "\n",
        "    quantization_dataset = nncf.Dataset(data_loader, transform_fn)\n",
        "\n",
        "    validation_fn = partial(validation_ac, validator=validator_ac)\n",
        "\n",
        "    quantized_model_ac = nncf.quantize_with_accuracy_control(\n",
        "        model,\n",
        "        quantization_dataset,\n",
        "        quantization_dataset,\n",
        "        validation_fn=validation_fn,\n",
        "        max_drop=0.003,\n",
        "        preset=nncf.QuantizationPreset.MIXED,\n",
        "        ignored_scope=nncf.IgnoredScope(\n",
        "            types=[\"Multiply\", \"Subtract\", \"Sigmoid\"],  # ignore operations\n",
        "            subgraphs=[\n",
        "                nncf.Subgraph(\n",
        "                    inputs=[\n",
        "                        \"/model.22/Concat_3\",\n",
        "                        \"/model.22/Concat_6\",\n",
        "                        \"/model.22/Concat_5\",\n",
        "                        \"/model.22/Concat_4\",\n",
        "                    ],\n",
        "                    outputs=[\"output0\"],\n",
        "                )\n",
        "            ],\n",
        "        ),\n",
        "    )\n",
        "    return quantized_model_ac\n",
        "\n",
        "\n",
        "def main():\n",
        "    MODEL_NAME = \"yolov8n-seg\"\n",
        "\n",
        "    model = YOLO(f\"{ROOT}/{MODEL_NAME}.pt\")\n",
        "    args = get_cfg(cfg=DEFAULT_CFG)\n",
        "    args.data = \"coco128-seg.yaml\"\n",
        "\n",
        "    # Prepare validation dataset and helper\n",
        "    validator, data_loader = prepare_validation(model, args)\n",
        "\n",
        "    # Convert to OpenVINO model\n",
        "    ov_model, ov_model_path = prepare_openvino_model(model, MODEL_NAME)\n",
        "\n",
        "    # Quantize mode in OpenVINO representation\n",
        "    quantized_model = quantize_ac(ov_model, data_loader, validator)\n",
        "\n",
        "    quantized_model_path = Path(f\"{ROOT}/{MODEL_NAME}_openvino_model/{MODEL_NAME}_quantized.xml\")\n",
        "    ov.save_model(quantized_model, str(quantized_model_path))\n",
        "\n",
        "    # Validate FP32 model\n",
        "    fp_stats, total_images, total_objects = validate(ov_model, tqdm(data_loader), validator)\n",
        "    print(\"Floating-point model validation results:\")\n",
        "    print_statistics(fp_stats, total_images, total_objects)\n",
        "\n",
        "    # Validate quantized model\n",
        "    q_stats, total_images, total_objects = validate(quantized_model, tqdm(data_loader), validator)\n",
        "    print(\"Quantized model validation results:\")\n",
        "    print_statistics(q_stats, total_images, total_objects)\n",
        "\n",
        "    # Benchmark performance of FP32 model\n",
        "    fp_model_perf = benchmark_performance(ov_model_path, args)\n",
        "    print(f\"Floating-point model performance: {fp_model_perf} FPS\")\n",
        "\n",
        "    # Benchmark performance of quantized model\n",
        "    quantized_model_perf = benchmark_performance(quantized_model_path, args)\n",
        "    print(f\"Quantized model performance: {quantized_model_perf} FPS\")\n",
        "\n",
        "    return fp_stats[\"metrics/mAP50-95(B)\"], q_stats[\"metrics/mAP50-95(B)\"], fp_model_perf, quantized_model_perf\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "rx9kIJM_lYKi",
        "outputId": "4249d467-0206-40a8-91e3-7a187ab194ba"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Calculating ranking scores \u001b[38;2;249;38;114m━━━━━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 20%\u001b[0m \u001b[38;2;0;104;181m11/55\u001b[0m • \u001b[38;2;0;104;181m0:08:54\u001b[0m • \u001b[38;2;0;104;181m-:--:--\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Calculating ranking scores <span style=\"color: #f92672; text-decoration-color: #f92672\">━━━━━━━━━━━</span><span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #800080; text-decoration-color: #800080\"> 20%</span> <span style=\"color: #0068b5; text-decoration-color: #0068b5\">11/55</span> • <span style=\"color: #0068b5; text-decoration-color: #0068b5\">0:08:54</span> • <span style=\"color: #0068b5; text-decoration-color: #0068b5\">-:--:--</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/content/OpenVino_Learn.ipynb\u001b[0m in \u001b[0;36m<cell line: 237>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/OpenVino_Learn.ipynb\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;31m# Quantize mode in OpenVINO representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m     \u001b[0mquantized_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquantize_ac\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mov_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0mquantized_model_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{ROOT}/{MODEL_NAME}_openvino_model/{MODEL_NAME}_quantized.xml\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/OpenVino_Learn.ipynb\u001b[0m in \u001b[0;36mquantize_ac\u001b[0;34m(model, data_loader, validator_ac)\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0mvalidation_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_ac\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidator_ac\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m     quantized_model_ac = nncf.quantize_with_accuracy_control(\n\u001b[0m\u001b[1;32m    173\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0mquantization_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nncf/quantization/quantize_model.py\u001b[0m in \u001b[0;36mquantize_with_accuracy_control\u001b[0;34m(model, calibration_dataset, validation_dataset, validation_fn, max_drop, drop_type, preset, target_device, subset_size, fast_bias_correction, model_type, ignored_scope, advanced_quantization_parameters, advanced_accuracy_restorer_parameters)\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnncf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopenvino\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantize_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mquantize_with_accuracy_control_impl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m         return quantize_with_accuracy_control_impl(\n\u001b[0m\u001b[1;32m    290\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0mcalibration_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nncf/openvino/quantization/quantize_model.py\u001b[0m in \u001b[0;36mquantize_with_accuracy_control_impl\u001b[0;34m(model, calibration_dataset, validation_dataset, validation_fn, max_drop, drop_type, preset, target_device, subset_size, fast_bias_correction, model_type, ignored_scope, advanced_quantization_parameters, advanced_accuracy_restorer_parameters)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0mval_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrap_validation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m     return quantize_with_accuracy_control_fn(\n\u001b[0m\u001b[1;32m    383\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mcalibration_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nncf/telemetry/decorator.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                         )\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m                 \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcategory\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcategory\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mprevious_category\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nncf/openvino/quantization/quantize_model.py\u001b[0m in \u001b[0;36mnative_quantize_with_accuracy_control_impl\u001b[0;34m(model, calibration_dataset, validation_dataset, validation_fn, max_drop, drop_type, preset, target_device, subset_size, fast_bias_correction, model_type, ignored_scope, advanced_quantization_parameters, advanced_accuracy_restorer_parameters)\u001b[0m\n\u001b[1;32m    273\u001b[0m             \u001b[0madvanced_accuracy_restorer_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m         )\n\u001b[0;32m--> 275\u001b[0;31m         quantized_model = accuracy_restorer.apply(\n\u001b[0m\u001b[1;32m    276\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m             \u001b[0minitial_metric_results\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nncf/quantization/algorithms/accuracy_control/algorithm.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, initial_model, initial_metric_results, quantized_model, quantized_metric_results, validation_dataset, validation_dataset_size, evaluator)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;31m# Accuracy drop is greater than the maximum drop so we need to restore accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m         return self._apply(\n\u001b[0m\u001b[1;32m    215\u001b[0m             \u001b[0minitial_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0minitial_metric_results\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nncf/quantization/algorithms/accuracy_control/algorithm.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, initial_model, initial_metric_results, quantized_model, quantized_metric_results, validation_dataset, validation_dataset_size, evaluator, accuracy_drop, algo_backend)\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0mranker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRanker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mranking_subset_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malgo_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_ranking_workers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0mgroups_to_rank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mranker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_groups_of_quantizers_to_rank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquantized_model_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m         ranked_groups = ranker.rank_groups_of_quantizers(\n\u001b[0m\u001b[1;32m    290\u001b[0m             \u001b[0mgroups_to_rank\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0mquantized_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nncf/quantization/algorithms/accuracy_control/ranker.py\u001b[0m in \u001b[0;36mrank_groups_of_quantizers\u001b[0;34m(self, groups_to_rank, quantized_model, quantized_model_graph, reference_values_for_each_item, approximate_values_for_each_item)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m                 ranking_scores = self._sequential_calculation_ranking_score(\n\u001b[0m\u001b[1;32m    179\u001b[0m                     \u001b[0mquantized_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m                     \u001b[0mquantized_model_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nncf/quantization/algorithms/accuracy_control/ranker.py\u001b[0m in \u001b[0;36m_sequential_calculation_ranking_score\u001b[0;34m(self, quantized_model, quantized_model_graph, groups_to_rank, ranking_subset_indices, reference_values_for_each_item)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0mprepared_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodified_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m             ranking_score = self._calculate_ranking_score(\n\u001b[0m\u001b[1;32m    214\u001b[0m                 \u001b[0mprepared_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mranking_subset_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreference_values_for_each_item\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nncf/quantization/algorithms/accuracy_control/ranker.py\u001b[0m in \u001b[0;36m_calculate_ranking_score\u001b[0;34m(self, prepared_model, ranking_subset_indices, reference_values_for_each_item)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_metric_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m             \u001b[0;31m# Calculate ranking score based on metric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m             ranking_score, _ = self._evaluator.validate_prepared_model(\n\u001b[0m\u001b[1;32m    279\u001b[0m                 \u001b[0mprepared_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mranking_subset_indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nncf/quantization/algorithms/accuracy_control/evaluator.py\u001b[0m in \u001b[0;36mvalidate_prepared_model\u001b[0;34m(self, prepared_model, dataset, indices)\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0mvalidation_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIterationCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m         \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues_for_each_item\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepared_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_for_inference\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_passed_iterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_iterations\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_iteration_count\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nncf/openvino/quantization/quantize_model.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m         \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/OpenVino_Learn.ipynb\u001b[0m in \u001b[0;36mvalidation_ac\u001b[0;34m(compiled_model, validation_loader, validator, num_samples)\u001b[0m\n\u001b[1;32m    147\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m             \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompiled_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"img\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnum_outputs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m                 \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcompiled_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openvino/runtime/ie_api.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, share_inputs, share_outputs, decode_strings)\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_infer_request\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_infer_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m         return self._infer_request.infer(\n\u001b[0m\u001b[1;32m    389\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m             \u001b[0mshare_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshare_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openvino/runtime/ie_api.py\u001b[0m in \u001b[0;36minfer\u001b[0;34m(self, inputs, share_inputs, share_outputs, decode_strings)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOVDict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \"\"\"\n\u001b[0;32m--> 132\u001b[0;31m         return OVDict(super().infer(_data_dispatch(\n\u001b[0m\u001b[1;32m    133\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}